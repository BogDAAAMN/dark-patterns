{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T19:43:57.379375Z",
     "start_time": "2019-02-26T19:43:57.370206Z"
    }
   },
   "source": [
    "# Crawl Data Analysis: Clustering v2 (using text + location features)\n",
    "\n",
    "This notebook experiments with clustering using both text and location features of the data. It should be run on cycles - see `clustering.ipynb` for a reference of how to set this up. It's written for Python 2.7 and uses several packages (see `import` statements).\n",
    "\n",
    "**Key idea.** Intuitively, we think it would make sense to incorporate location features (e.g. x, y of segments, is it in a popup dialog?, etc). We distill these to the following binary features:\n",
    "\n",
    "- Is this segment in a popup?\n",
    "- Is this segment in the:\n",
    "    - x: left, middle, or right of the page\n",
    "    - y: top, middle, or bottom of the page\n",
    "    \n",
    "We use much of the logic from `dismiss_dialog.js` to determine whether a segment is in a popup, and we break the page into 3 sections in the x/y dimensions and place it in one of these \"bins\". e.g. A segment that's in the \"main\" content (center of the page) and not in a popup might have the representation: (0, 0, 1, 0, 0, 1, 0), where each feature is (popup, x_left, x_middle, x_right, y_top, y_middle, y_bottom)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T22:27:29.398119Z",
     "start_time": "2019-02-27T22:27:29.390715Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read from database and build feature matrix\n",
    "\n",
    "Preprocessing:\n",
    "\n",
    "- Lower case (but add an indicator for whether there were all-caps words)\n",
    "- Replace numbers with a placeholder\n",
    "- Remove uninteresting punctuation (e.g. keep exclamation points, but get rid of periods)\n",
    "- Remove excess whitespace\n",
    "- Remove \"stop words\"\n",
    "- Stem words (via Porter's stemming algorithm)\n",
    "\n",
    "Feature representation: bag-of-words (each feature is an indicator for a word) + indicator for whether the text contained all-caps words + the location features described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T22:27:29.618612Z",
     "start_time": "2019-02-27T22:27:29.458231Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /u/mjf4/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "ORIGINAL STRING:\n",
      "Color   \n",
      "Choose an option\n",
      "Silver\n",
      "Space Gray\n",
      "\n",
      "Size    \n",
      "Choose an option\n",
      "64 gb\n",
      "\n",
      "BUY NOW\n",
      "\n",
      "RESULT OF PREPROCESSING:\n",
      "(u'color choos option silver space gray size choos option dpnum gb buy', 1)\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = re.sub(r'[!$*%-?]', '', string.punctuation)\n",
    "stopwords = stopwords.union(set(punctuation))\n",
    "\n",
    "# Returns preprocessed version of the string s, as well as an indicator (0 or 1)\n",
    "# for whether s contained any all-caps words\n",
    "def preprocess(s):\n",
    "    s = re.sub(r'[%s]+' % string.whitespace, ' ', s)\n",
    "    \n",
    "    caps = 0\n",
    "    words = s.split()\n",
    "    for word in words:\n",
    "        if word.isupper():\n",
    "            caps = 1\n",
    "            break\n",
    "    \n",
    "    s = s.lower()\n",
    "    s = re.sub(r'\\d+', 'dpnum', s)\n",
    "    s = re.sub(r'[^a-z\\s]', '', s)\n",
    "    words = s.split()\n",
    "    words = [stemmer.stem(w) for w in words if len(w) > 0 and w not in stopwords]\n",
    "    \n",
    "    # Get rid of suffixes on numbers (e.g. units) and merge multiple numbers\n",
    "    # in one word together\n",
    "    for i in range(len(words)):\n",
    "        if words[i].startswith('dpnum'):\n",
    "            words[i] = 'dpnum'\n",
    "    \n",
    "    return ' '.join(words), caps\n",
    "\n",
    "# Sanity check\n",
    "s = '''Color   \n",
    "Choose an option\n",
    "Silver\n",
    "Space Gray\n",
    "\n",
    "Size    \n",
    "Choose an option\n",
    "64 gb\n",
    "\n",
    "BUY NOW'''\n",
    "print('ORIGINAL STRING:')\n",
    "print(s)\n",
    "print()\n",
    "print('RESULT OF PREPROCESSING:')\n",
    "print(preprocess(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through data, storing text-based features as well as additional columns, which we'll use to compute the extra features next.\n",
    "\n",
    "(Note that as we read data, we track the max values for `top` (y) and `left` (x) to estimate the height and width of each site, since we don't have the actual values.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T22:44:08.181022Z",
     "start_time": "2019-02-27T22:27:29.642104Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [00:00, 175.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:\n",
      "id (<type 'int'>)\n",
      "crawl_id (<type 'int'>)\n",
      "visit_id (<type 'int'>)\n",
      "node_name (<type 'unicode'>)\n",
      "node_id (<type 'int'>)\n",
      "top (<type 'int'>)\n",
      "left (<type 'int'>)\n",
      "width (<type 'int'>)\n",
      "height (<type 'int'>)\n",
      "style (<type 'unicode'>)\n",
      "inner_text (<type 'unicode'>)\n",
      "outer_html (<type 'unicode'>)\n",
      "longest_text (<type 'unicode'>)\n",
      "longest_text_width (<type 'int'>)\n",
      "longest_text_height (<type 'int'>)\n",
      "longest_text_top (<type 'int'>)\n",
      "longest_text_left (<type 'int'>)\n",
      "longest_text_style (<type 'unicode'>)\n",
      "num_buttons (<type 'int'>)\n",
      "num_imgs (<type 'int'>)\n",
      "num_anchors (<type 'unicode'>)\n",
      "time_stamp (<type 'unicode'>)\n",
      "visit_id (<type 'int'>)\n",
      "crawl_id (<type 'int'>)\n",
      "site_url (<type 'unicode'>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "740532it [16:30, 747.92it/s] \n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import sqlite3\n",
    "\n",
    "db = '/n/fs/darkpatterns/crawl/2018-12-08_segmentation_pilot2/2018-12-08_segmentation_pilot2.sqlite'\n",
    "con = sqlite3.connect(db)\n",
    "con.row_factory = sqlite3.Row\n",
    "cur = con.cursor()\n",
    "\n",
    "query = \"\"\"select * from\n",
    "    segments as sg left join site_visits as sv on sv.visit_id = sg.visit_id\n",
    "    where lower(sg.node_name) <> 'body' and sg.inner_text <> ''\n",
    "\"\"\"\n",
    "\n",
    "hv = HashingVectorizer(n_features=2**18, strip_accents='ascii', alternate_sign=False)\n",
    "text_fts = None\n",
    "extra_fts = None\n",
    "text_fts_tmp_list = []\n",
    "extra_fts_tmp_list = []\n",
    "widths = defaultdict(lambda: 0)\n",
    "heights = defaultdict(lambda: 0)\n",
    "count = 0\n",
    "for segment in tqdm(cur.execute(query)):\n",
    "    text_proc, caps = preprocess(segment['inner_text'])\n",
    "\n",
    "    if count == 0:\n",
    "        print('Columns:\\n%s' % '\\n'.join(['%s (%s)' % (c, str(type(segment[c]))) for c in segment.keys()]))\n",
    "    \n",
    "    # Store extra features\n",
    "    try:\n",
    "        style = json.loads(segment['style'])\n",
    "        extra_fts_tmp_list.append(np.array([segment['site_url'], caps, style['display'],\n",
    "            style['visibility'], style['position'], segment['left'], segment['top'],\n",
    "            style['z-index']]).reshape(1, 8))\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "    # Update width/height of this page\n",
    "    url = segment['site_url']\n",
    "    x = segment['left']\n",
    "    y = segment['top']\n",
    "    if x > widths[url]:\n",
    "        widths[url] = x\n",
    "    if y > heights[url]:\n",
    "        heights[url] = y\n",
    "        \n",
    "    # Compute text features\n",
    "    fts = hv.fit_transform([text_proc])\n",
    "    text_fts_tmp_list.append(fts)\n",
    "    \n",
    "    if count % 25000 == 0:\n",
    "        if count == 0:\n",
    "            text_fts = text_fts_tmp_list[0]\n",
    "            extra_fts = extra_fts_tmp_list[0]\n",
    "        else:\n",
    "            text_fts = sparse.vstack([text_fts] + text_fts_tmp_list)\n",
    "            extra_fts = np.vstack([extra_fts] + extra_fts_tmp_list)\n",
    "        text_fts_tmp_list = []\n",
    "        extra_fts_tmp_list = []\n",
    "    count += 1\n",
    "    \n",
    "text_fts = sparse.vstack([text_fts] + text_fts_tmp_list)\n",
    "extra_fts = np.vstack([extra_fts] + extra_fts_tmp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# text_fts_tfidf = TfidfTransformer().fit_transform(text_fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T15:44:58.327283Z",
     "start_time": "2019-02-28T15:44:58.318496Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text features matrix: (740532, 262144) (float64)\n",
      "extra features matrix: (740532, 8) (<U478)\n"
     ]
    }
   ],
   "source": [
    "print('text features matrix: %s (%s)' % (str(text_fts.shape), str(text_fts.dtype)))\n",
    "print('extra features matrix: %s (%s)' % (str(extra_fts.shape), str(extra_fts.dtype)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:41:54.840387Z",
     "start_time": "2019-02-28T00:41:54.720039Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Returns indicator (0 or 1)\n",
    "def is_in_popup(row, widths, heights):\n",
    "    display = (row[2] != 'none')\n",
    "    visibility = (row[3] == 'visible')\n",
    "    position = (row[4] != 'static')\n",
    "    zindex = row[7]\n",
    "    \n",
    "    # Check if segment is roughly in the center of the page\n",
    "    url = row[0]\n",
    "    x = int(row[5])\n",
    "    y = int(row[6])\n",
    "    x_center = widths[url] / 2.0\n",
    "    y_center = heights[url] / 2.0\n",
    "    thresh = 0.3\n",
    "    if np.abs(x - x_center) / widths[url] < thresh and np.abs(y - y_center) / heights[url] < thresh:\n",
    "        in_center = True\n",
    "    else:\n",
    "        in_center = False\n",
    "\n",
    "    if display and visibility and position and (zindex != 'auto') and (zindex > 0) and in_center:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:41:54.869599Z",
     "start_time": "2019-02-28T00:41:54.845927Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Returns three indicators (0 or 1) for whether this segment is in the\n",
    "# left, middle, or right section of the page\n",
    "def compute_x_fts(row, widths):\n",
    "    url = row[0]\n",
    "    x = int(row[5])\n",
    "    width = widths[url]\n",
    "    if x < 0.2 * width:\n",
    "        return 1, 0, 0\n",
    "    elif x > 0.8 * width:\n",
    "        return 0, 0, 1\n",
    "    else:\n",
    "        return 0, 1, 0\n",
    "\n",
    "# Analogous for top, middle, bottom sections\n",
    "def compute_y_fts(row, widths):\n",
    "    url = row[0]\n",
    "    y = int(row[6])\n",
    "    height = heights[url]\n",
    "    if y < 0.1 * height:\n",
    "        return 1, 0, 0\n",
    "    elif y > 0.9 * height:\n",
    "        return 0, 0, 1\n",
    "    else:\n",
    "        return 0, 1, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:42:27.602522Z",
     "start_time": "2019-02-28T00:41:54.875735Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/fs/ugrad/ug19/mjf4/.local/lib/python2.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  from ipykernel import kernelapp as app\n",
      "/n/fs/ugrad/ug19/mjf4/.local/lib/python2.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "urls = np.matrix(extra_fts[:,0]).T\n",
    "\n",
    "x_fts = np.matrix([compute_x_fts(row, widths) for row in extra_fts])\n",
    "y_fts = np.matrix([compute_y_fts(row, heights) for row in extra_fts])\n",
    "popup_fts = np.matrix([is_in_popup(row, widths, heights) for row in extra_fts]).T\n",
    "\n",
    "caps_fts = np.matrix([int(row) for row in extra_fts[:,1]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T01:40:05.055011Z",
     "start_time": "2019-02-28T01:40:05.044463Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_fts matrix: (740532, 262144) (float64)\n",
      "urls matrix: (740532, 1) (<U478)\n",
      "x_fts matrix: (740532, 3) (int64)\n",
      "y_fts matrix: (740532, 3) (int64)\n",
      "popup_fts matrix: (740532, 1) (int64)\n",
      "caps_fts matrix: (740532, 1) (int64)\n"
     ]
    }
   ],
   "source": [
    "matrices = [('text_fts', text_fts), ('urls', urls), ('x_fts', x_fts), ('y_fts', y_fts),\n",
    "    ('popup_fts', popup_fts), ('caps_fts', caps_fts)]\n",
    "for name, m in matrices:\n",
    "    print('%s matrix: %s (%s)' % (name, str(m.shape), str(m.dtype)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Text + location features -> PCA\n",
    "\n",
    "On the *reduced dataset*, try adding the location features to the bag-of-words text features. Then apply PCA and compare the PCs. Do these new features add enough variance to be accounted for in the PCs? If so, we can try proceeding with clustering and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:42:27.913808Z",
     "start_time": "2019-02-28T00:42:27.665030Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "fts = sparse.hstack([text_fts, caps_fts, popup_fts, x_fts, y_fts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the columns have been added properly - there should be 8 extra columns in addition to text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:42:27.928700Z",
     "start_time": "2019-02-28T00:42:27.920778Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text features matrix: (740532, 262144) (float64)\n",
      "features matrix: (740532, 262152) (float64)\n"
     ]
    }
   ],
   "source": [
    "print('text features matrix: %s (%s)' % (str(text_fts.shape), str(text_fts.dtype)))\n",
    "print('features matrix: %s (%s)' % (str(fts.shape), str(fts.dtype)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do PCA with and without the new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:46:17.003013Z",
     "start_time": "2019-02-28T00:42:27.934417Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing SVD with additional features...\n",
      "Computing SVD without additional features...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=200, n_iter=5,\n",
       "       random_state=13, tol=0.0)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy import stats\n",
    "\n",
    "print('Computing SVD with additional features...')\n",
    "svd_extra = TruncatedSVD(n_components=200, random_state=13)\n",
    "svd_extra.fit(fts)\n",
    "\n",
    "print('Computing SVD without additional features...')\n",
    "fts_no_extra = sparse.hstack([fts.tocsc()[:,:-8], np.zeros((fts.shape[0], 8))])\n",
    "svd_no_extra = TruncatedSVD(n_components=200, random_state=13)\n",
    "svd_no_extra.fit(fts_no_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:46:17.012853Z",
     "start_time": "2019-02-28T00:46:17.007610Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix of PCs (should be same size): (200, 262152) vs (200, 262152)\n"
     ]
    }
   ],
   "source": [
    "print('Matrix of PCs (should be same size): %s vs %s' % (str(svd_extra.components_.shape), str(svd_no_extra.components_.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the PCs for both sets of data (with the new features and without), we'll compare *cosine angles* between the two sets of PCs to get a sense of how similar or different they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:59:29.411272Z",
     "start_time": "2019-02-28T00:59:27.943684Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    200.000000\n",
       "mean       1.004655\n",
       "std        0.054193\n",
       "min        0.658319\n",
       "25%        0.987769\n",
       "50%        1.006368\n",
       "75%        1.030209\n",
       "max        1.127439\n",
       "dtype: float64"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "ncomps = svd_extra.components_.shape[0]\n",
    "cosines = [cosine(svd_extra.components_[i], svd_no_extra.components_[i]) for i in range(ncomps)]\n",
    "pd.Series(cosines).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So based on this, we conclude that adding these new features doesn't change the PCs very much at all (cosine similarities are hovering tightly around 1), so:\n",
    "1. The variance in the data is dominated by the text features (as we would expect, since they account for $2^{18}$ of the features)\n",
    "2. It's not worth clustering on this particular feature representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Text features -> PCA -> + location features\n",
    "\n",
    "Alternatively, we can add these new features after applying PCA to the text features *alone*. Thus we would get a concise representation of the text, and the new features could contribute a larger share of the information in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T01:18:38.397075Z",
     "start_time": "2019-02-28T01:16:42.953640Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=200, random_state=13)\n",
    "fts_reduced = svd.fit_transform(text_fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T15:00:57.762392Z",
     "start_time": "2019-02-28T15:00:57.755606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced text features matrix: (740532, 200)\n"
     ]
    }
   ],
   "source": [
    "print('Reduced text features matrix: %s' % str(fts_reduced.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T15:02:29.950816Z",
     "start_time": "2019-02-28T15:02:28.805602Z"
    }
   },
   "outputs": [],
   "source": [
    "fts = np.hstack([fts_reduced, caps_fts, popup_fts, x_fts, y_fts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T15:03:55.925007Z",
     "start_time": "2019-02-28T15:03:55.918251Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features matrix (should have 8 more features than fts_reduced): (740532, 208) (float64)\n"
     ]
    }
   ],
   "source": [
    "print('Features matrix (should have 8 more features than fts_reduced): %s (%s)' % (str(fts.shape), str(fts.dtype)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just inspect each feature to get a sense of their scale - before we cluster, we want to be sure that the extra features don't get proportionally more/less weight because of their scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T15:24:39.994210Z",
     "start_time": "2019-02-28T15:24:39.000604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of a sample of text features:\n",
      "count    740532.000000\n",
      "mean          0.000299\n",
      "std           0.020636\n",
      "min          -0.293505\n",
      "25%          -0.000167\n",
      "50%           0.000089\n",
      "75%           0.000516\n",
      "max           0.428637\n",
      "dtype: float64 \n",
      "\n",
      "count    740532.000000\n",
      "mean          0.000362\n",
      "std           0.035726\n",
      "min          -0.396693\n",
      "25%          -0.001822\n",
      "50%           0.000000\n",
      "75%           0.000623\n",
      "max           0.588021\n",
      "dtype: float64 \n",
      "\n",
      "count    740532.000000\n",
      "mean         -0.000263\n",
      "std           0.030570\n",
      "min          -0.179388\n",
      "25%          -0.003342\n",
      "50%          -0.000032\n",
      "75%           0.000058\n",
      "max           0.928130\n",
      "dtype: float64 \n",
      "\n",
      "count    740532.000000\n",
      "mean          0.000633\n",
      "std           0.038021\n",
      "min          -0.334300\n",
      "25%          -0.001493\n",
      "50%          -0.000023\n",
      "75%           0.000935\n",
      "max           0.492632\n",
      "dtype: float64 \n",
      "\n",
      "count    740532.000000\n",
      "mean          0.000200\n",
      "std           0.027739\n",
      "min          -0.409735\n",
      "25%          -0.001801\n",
      "50%          -0.000086\n",
      "75%           0.000140\n",
      "max           0.613431\n",
      "dtype: float64 \n",
      "\n",
      "count    740532.000000\n",
      "mean          0.000136\n",
      "std           0.029145\n",
      "min          -0.484741\n",
      "25%          -0.000742\n",
      "50%          -0.000071\n",
      "75%           0.000356\n",
      "max           0.567859\n",
      "dtype: float64 \n",
      "\n",
      "count    740532.000000\n",
      "mean         -0.000140\n",
      "std           0.020658\n",
      "min          -0.288873\n",
      "25%          -0.000725\n",
      "50%          -0.000022\n",
      "75%           0.000184\n",
      "max           0.313299\n",
      "dtype: float64 \n",
      "\n",
      "count    740532.000000\n",
      "mean          0.003119\n",
      "std           0.049401\n",
      "min          -0.339823\n",
      "25%          -0.001119\n",
      "50%           0.000000\n",
      "75%           0.000577\n",
      "max           0.851051\n",
      "dtype: float64 \n",
      "\n",
      "count    740532.000000\n",
      "mean          0.000436\n",
      "std           0.028364\n",
      "min          -0.163618\n",
      "25%          -0.005711\n",
      "50%          -0.000000\n",
      "75%           0.000268\n",
      "max           0.660446\n",
      "dtype: float64 \n",
      "\n",
      "count    740532.000000\n",
      "mean          0.000521\n",
      "std           0.025302\n",
      "min          -0.201997\n",
      "25%          -0.000032\n",
      "50%           0.000025\n",
      "75%           0.000293\n",
      "max           0.807362\n",
      "dtype: float64 \n",
      "\n",
      "Distribution of nontext features:\n",
      "count    740532.000000\n",
      "mean          0.412652\n",
      "std           0.492312\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           1.000000\n",
      "max           1.000000\n",
      "dtype: float64 \n",
      "\n",
      "count    740532.000000\n",
      "mean          0.029892\n",
      "std           0.170290\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           0.000000\n",
      "max           1.000000\n",
      "dtype: float64 \n",
      "\n",
      "count    740532.000000\n",
      "mean          0.323484\n",
      "std           0.467806\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           1.000000\n",
      "max           1.000000\n",
      "dtype: float64 \n",
      "\n",
      "count    740532.000000\n",
      "mean          0.458534\n",
      "std           0.498278\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           1.000000\n",
      "max           1.000000\n",
      "dtype: float64 \n",
      "\n",
      "count    740532.000000\n",
      "mean          0.217982\n",
      "std           0.412876\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           0.000000\n",
      "max           1.000000\n",
      "dtype: float64 \n",
      "\n",
      "count    740532.000000\n",
      "mean          0.378320\n",
      "std           0.484968\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           1.000000\n",
      "max           1.000000\n",
      "dtype: float64 \n",
      "\n",
      "count    740532.000000\n",
      "mean          0.582520\n",
      "std           0.493144\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           1.000000\n",
      "75%           1.000000\n",
      "max           1.000000\n",
      "dtype: float64 \n",
      "\n",
      "count    740532.000000\n",
      "mean          0.039160\n",
      "std           0.193975\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           0.000000\n",
      "max           1.000000\n",
      "dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Distribution of a sample of text features:')\n",
    "subset_idxs = np.random.choice(np.arange(fts.shape[1] - 8), size=10, replace=False)\n",
    "for i in subset_idxs:\n",
    "    s = pd.Series(np.ravel(fts[:,i])).describe()\n",
    "    print(str(s), '\\n')\n",
    "    \n",
    "print('Distribution of nontext features:')\n",
    "for i in range(fts.shape[1] - 8, fts.shape[1]):\n",
    "    s = pd.Series(np.ravel(fts[:,i])).describe()\n",
    "    print(str(s), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the range of values of the text features is roughly between -1 and 1, whereas the nontext features are between 0 and 1 (naturally, since they're binary). I'm assuming this should be fine, since the nontext features are roughly the same order of magnitude as the text features, so I believe neither is getting bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "We'll proceed with clustering based on the feature matrix resulting from experiment 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
