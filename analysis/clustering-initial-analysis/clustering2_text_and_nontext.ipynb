{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T19:43:57.379375Z",
     "start_time": "2019-02-26T19:43:57.370206Z"
    }
   },
   "source": [
    "# Crawl Data Analysis: Clustering v2 (using text + nontext features)\n",
    "\n",
    "This notebook experiments with clustering using both text and nontext features of the data. It assumes it's running on cycles - see `clustering.ipynb` for a reference of how to set this notebook up to run remotely on cycles.\n",
    "\n",
    "This notebook is written for Python 2.7 and uses several packages (see `import` statements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T22:27:29.398119Z",
     "start_time": "2019-02-27T22:27:29.390715Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: Physical location features\n",
    "\n",
    "Intuitively, we think it would make sense to incorporate location features (e.g. x, y of segments, is it in a popup dialog?, etc). We distill these to the following binary features:\n",
    "\n",
    "- Is this segment in a popup?\n",
    "- Is this segment in the:\n",
    "    - x: left, middle, or right of the page\n",
    "    - y: top, middle, or bottom of the page\n",
    "    \n",
    "We use much of the logic from `dismiss_dialog.js` to determine whether a segment is in a popup, and we break the page into 3 sections in the x/y dimensions and place it in one of these \"bins\". e.g. A segment that's in the \"main\" content (center of the page) and not in a popup might have the representation: (0, 0, 1, 0, 0, 1, 0), where each feature is (popup, x_left, x_middle, x_right, y_top, y_middle, y_bottom).\n",
    "\n",
    "**Experiment**: On the *reduced dataset*, try adding these features to a bag-of-words representation. Then apply PCA and compare the PCs. Do these new features add enough variance to be accounted for in the PCs? If so, we can try proceeding with clustering and see the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from database and build feature matrix\n",
    "\n",
    "Preprocessing:\n",
    "\n",
    "- Lower case (but add an indicator for whether there were all-caps words)\n",
    "- Replace numbers with a placeholder\n",
    "- Remove uninteresting punctuation (e.g. keep exclamation points, but get rid of periods)\n",
    "- Remove excess whitespace\n",
    "- Remove \"stop words\"\n",
    "- Stem words (via Porter's stemming algorithm)\n",
    "\n",
    "Feature representation: bag-of-words (each feature is an indicator for a word) + indicator for whether the text contained all-caps words + the location features described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T22:27:29.618612Z",
     "start_time": "2019-02-27T22:27:29.458231Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /u/mjf4/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "ORIGINAL STRING:\n",
      "Color   \n",
      "Choose an option\n",
      "Silver\n",
      "Space Gray\n",
      "\n",
      "Size    \n",
      "Choose an option\n",
      "64 gb\n",
      "\n",
      "BUY NOW\n",
      "\n",
      "RESULT OF PREPROCESSING:\n",
      "(u'color choos option silver space gray size choos option dpnum gb buy', 1)\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = re.sub(r'[!$*%-?]', '', string.punctuation)\n",
    "stopwords = stopwords.union(set(punctuation))\n",
    "\n",
    "# Returns preprocessed version of the string s, as well as an indicator (0 or 1)\n",
    "# for whether s contained any all-caps words\n",
    "def preprocess(s):\n",
    "    s = re.sub(r'[%s]+' % string.whitespace, ' ', s)\n",
    "    \n",
    "    caps = 0\n",
    "    words = s.split()\n",
    "    for word in words:\n",
    "        if word.isupper():\n",
    "            caps = 1\n",
    "            break\n",
    "    \n",
    "    s = s.lower()\n",
    "    s = re.sub(r'\\d+', 'dpnum', s)\n",
    "    s = re.sub(r'[^a-z\\s]', '', s)\n",
    "    words = s.split()\n",
    "    words = [stemmer.stem(w) for w in words if len(w) > 0 and w not in stopwords]\n",
    "    \n",
    "    # Get rid of suffixes on numbers (e.g. units) and merge multiple numbers\n",
    "    # in one word together\n",
    "    for i in range(len(words)):\n",
    "        if words[i].startswith('dpnum'):\n",
    "            words[i] = 'dpnum'\n",
    "    \n",
    "    return ' '.join(words), caps\n",
    "\n",
    "# Sanity check\n",
    "s = '''Color   \n",
    "Choose an option\n",
    "Silver\n",
    "Space Gray\n",
    "\n",
    "Size    \n",
    "Choose an option\n",
    "64 gb\n",
    "\n",
    "BUY NOW'''\n",
    "print('ORIGINAL STRING:')\n",
    "print(s)\n",
    "print()\n",
    "print('RESULT OF PREPROCESSING:')\n",
    "print(preprocess(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through data, storing text-based features as well as additional columns, which we'll use to compute the extra features next.\n",
    "\n",
    "(Note that as we read data, we track the max values for `top` (y) and `left` (x) to estimate the height and width of each site, since we don't have the actual values.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T22:44:08.181022Z",
     "start_time": "2019-02-27T22:27:29.642104Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [00:00, 175.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:\n",
      "id (<type 'int'>)\n",
      "crawl_id (<type 'int'>)\n",
      "visit_id (<type 'int'>)\n",
      "node_name (<type 'unicode'>)\n",
      "node_id (<type 'int'>)\n",
      "top (<type 'int'>)\n",
      "left (<type 'int'>)\n",
      "width (<type 'int'>)\n",
      "height (<type 'int'>)\n",
      "style (<type 'unicode'>)\n",
      "inner_text (<type 'unicode'>)\n",
      "outer_html (<type 'unicode'>)\n",
      "longest_text (<type 'unicode'>)\n",
      "longest_text_width (<type 'int'>)\n",
      "longest_text_height (<type 'int'>)\n",
      "longest_text_top (<type 'int'>)\n",
      "longest_text_left (<type 'int'>)\n",
      "longest_text_style (<type 'unicode'>)\n",
      "num_buttons (<type 'int'>)\n",
      "num_imgs (<type 'int'>)\n",
      "num_anchors (<type 'unicode'>)\n",
      "time_stamp (<type 'unicode'>)\n",
      "visit_id (<type 'int'>)\n",
      "crawl_id (<type 'int'>)\n",
      "site_url (<type 'unicode'>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "740532it [16:30, 747.92it/s] \n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import sqlite3\n",
    "\n",
    "db = '/n/fs/darkpatterns/crawl/2018-12-08_segmentation_pilot2/2018-12-08_segmentation_pilot2.sqlite'\n",
    "con = sqlite3.connect(db)\n",
    "con.row_factory = sqlite3.Row\n",
    "cur = con.cursor()\n",
    "\n",
    "query = \"\"\"select * from\n",
    "    segments as sg left join site_visits as sv on sv.visit_id = sg.visit_id\n",
    "    where lower(sg.node_name) <> 'body' and sg.inner_text <> ''\n",
    "\"\"\"\n",
    "\n",
    "hv = HashingVectorizer(n_features=2**18, strip_accents='ascii', alternate_sign=False)\n",
    "text_fts = None\n",
    "extra_fts = None\n",
    "text_fts_tmp_list = []\n",
    "extra_fts_tmp_list = []\n",
    "widths = defaultdict(lambda: 0)\n",
    "heights = defaultdict(lambda: 0)\n",
    "count = 0\n",
    "for segment in tqdm(cur.execute(query)):\n",
    "    text_proc, caps = preprocess(segment['inner_text'])\n",
    "\n",
    "    if count == 0:\n",
    "        print('Columns:\\n%s' % '\\n'.join(['%s (%s)' % (c, str(type(segment[c]))) for c in segment.keys()]))\n",
    "    \n",
    "    # Store extra features\n",
    "    try:\n",
    "        style = json.loads(segment['style'])\n",
    "        extra_fts_tmp_list.append(np.array([segment['site_url'], caps, style['display'],\n",
    "            style['visibility'], style['position'], segment['left'], segment['top'],\n",
    "            style['z-index']]).reshape(1, 8))\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "    # Update width/height of this page\n",
    "    url = segment['site_url']\n",
    "    x = segment['left']\n",
    "    y = segment['top']\n",
    "    if x > widths[url]:\n",
    "        widths[url] = x\n",
    "    if y > heights[url]:\n",
    "        heights[url] = y\n",
    "        \n",
    "    # Compute text features\n",
    "    fts = hv.fit_transform([text_proc])\n",
    "    text_fts_tmp_list.append(fts)\n",
    "    \n",
    "    if count % 25000 == 0:\n",
    "        if count == 0:\n",
    "            text_fts = text_fts_tmp_list[0]\n",
    "            extra_fts = extra_fts_tmp_list[0]\n",
    "        else:\n",
    "            text_fts = sparse.vstack([text_fts] + text_fts_tmp_list)\n",
    "            extra_fts = np.vstack([extra_fts] + extra_fts_tmp_list)\n",
    "        text_fts_tmp_list = []\n",
    "        extra_fts_tmp_list = []\n",
    "    count += 1\n",
    "    \n",
    "text_fts = sparse.vstack([text_fts] + text_fts_tmp_list)\n",
    "extra_fts = np.vstack([extra_fts] + extra_fts_tmp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "text_fts_tfidf = TfidfTransformer().fit_transform(text_fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:41:54.714540Z",
     "start_time": "2019-02-28T00:41:54.704829Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text features matrix: (740532, 262144) (float64)\n",
      "extra features matrix: (740532, 8) (<U478)\n",
      "[[u'http://1-pharm.com/categories/ED-Sample-Packs/ED-Sample-Pack-2' u'1'\n",
      "  u'block' u'visible' u'absolute' u'127' u'4' u'auto']\n",
      " [u'http://1-pharm.com/categories/ED-Sample-Packs/ED-Sample-Pack-2' u'0'\n",
      "  u'inline' u'visible' u'static' u'156' u'24' u'auto']\n",
      " [u'http://1-pharm.com/categories/ED-Sample-Packs/ED-Sample-Pack-2' u'0'\n",
      "  u'block' u'visible' u'absolute' u'128' u'222' u'240']\n",
      " [u'http://1-pharm.com/categories/ED-Sample-Packs/ED-Sample-Pack-2' u'0'\n",
      "  u'inline-block' u'visible' u'static' u'695' u'154' u'auto']\n",
      " [u'http://1-pharm.com/categories/ED-Sample-Packs/ED-Sample-Pack-2' u'1'\n",
      "  u'block' u'visible' u'absolute' u'871' u'232' u'auto']]\n"
     ]
    }
   ],
   "source": [
    "print('text features matrix: %s (%s)' % (str(text_fts.shape), str(text_fts.dtype)))\n",
    "print('extra features matrix: %s (%s)' % (str(extra_fts.shape), str(extra_fts.dtype)))\n",
    "print(extra_fts[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:41:54.840387Z",
     "start_time": "2019-02-28T00:41:54.720039Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# Returns indicator (0 or 1)\n",
    "def is_in_popup(row, widths, heights):\n",
    "    display = (row[2] != 'none')\n",
    "    visibility = (row[3] == 'visible')\n",
    "    position = (row[4] != 'static')\n",
    "    zindex = row[7]\n",
    "    \n",
    "    # Check if segment is roughly in the center of the page\n",
    "    url = row[0]\n",
    "    x = int(row[5])\n",
    "    y = int(row[6])\n",
    "    x_center = widths[url] / 2.0\n",
    "    y_center = heights[url] / 2.0\n",
    "    thresh = 0.3\n",
    "    if np.abs(x - x_center) / widths[url] < thresh and np.abs(y - y_center) / heights[url] < thresh:\n",
    "        in_center = True\n",
    "    else:\n",
    "        in_center = False\n",
    "\n",
    "    if display and visibility and position and (zindex != 'auto') and (zindex > 0) and in_center:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:41:54.869599Z",
     "start_time": "2019-02-28T00:41:54.845927Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# Returns three indicators (0 or 1) for whether this segment is in the\n",
    "# left, middle, or right section of the page\n",
    "def compute_x_fts(row, widths):\n",
    "    url = row[0]\n",
    "    x = int(row[5])\n",
    "    width = widths[url]\n",
    "    if x < 0.2 * width:\n",
    "        return 1, 0, 0\n",
    "    elif x > 0.8 * width:\n",
    "        return 0, 0, 1\n",
    "    else:\n",
    "        return 0, 1, 0\n",
    "\n",
    "# Analogous for top, middle, bottom sections\n",
    "def compute_y_fts(row, widths):\n",
    "    url = row[0]\n",
    "    y = int(row[6])\n",
    "    height = heights[url]\n",
    "    if y < 0.1 * height:\n",
    "        return 1, 0, 0\n",
    "    elif y > 0.9 * height:\n",
    "        return 0, 0, 1\n",
    "    else:\n",
    "        return 0, 1, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:42:27.602522Z",
     "start_time": "2019-02-28T00:41:54.875735Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/fs/ugrad/ug19/mjf4/.local/lib/python2.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  from ipykernel import kernelapp as app\n",
      "/n/fs/ugrad/ug19/mjf4/.local/lib/python2.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "urls = np.matrix(extra_fts[:,0]).T\n",
    "\n",
    "x_fts = np.matrix([compute_x_fts(row, widths) for row in extra_fts])\n",
    "y_fts = np.matrix([compute_y_fts(row, heights) for row in extra_fts])\n",
    "popup_fts = np.matrix([is_in_popup(row, widths, heights) for row in extra_fts]).T\n",
    "\n",
    "caps_fts = np.matrix([int(row) for row in extra_fts[:,1]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:42:27.658410Z",
     "start_time": "2019-02-28T00:42:27.607249Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_fts matrix: (740532, 262144) (float64)\n",
      "urls matrix: (740532, 1) (<U478)\n",
      "x_fts matrix: (740532, 3) (int64)\n",
      "y_fts matrix: (740532, 3) (int64)\n",
      "popup_fts matrix: (740532, 1) (int64)\n",
      "caps_fts matrix: (740532, 1) (int64)\n"
     ]
    }
   ],
   "source": [
    "matrices = [('text_fts', text_fts), ('urls', urls), ('x_fts', x_fts), ('y_fts', y_fts),\n",
    "    ('popup_fts', popup_fts), ('caps_fts', caps_fts)]\n",
    "for name, m in matrices:\n",
    "    print('%s matrix: %s (%s)' % (name, str(m.shape), str(m.dtype)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:42:27.913808Z",
     "start_time": "2019-02-28T00:42:27.665030Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "fts = sparse.hstack([text_fts, caps_fts, popup_fts, x_fts, y_fts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the columns have been added properly - there should be 8 extra columns in addition to text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:42:27.928700Z",
     "start_time": "2019-02-28T00:42:27.920778Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text features matrix: (740532, 262144) (float64)\n",
      "features matrix: (740532, 262152) (float64)\n"
     ]
    }
   ],
   "source": [
    "print('text features matrix: %s (%s)' % (str(text_fts.shape), str(text_fts.dtype)))\n",
    "print('features matrix: %s (%s)' % (str(fts.shape), str(fts.dtype)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:46:17.003013Z",
     "start_time": "2019-02-28T00:42:27.934417Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing SVD with additional features...\n",
      "Computing SVD without additional features...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=200, n_iter=5,\n",
       "       random_state=13, tol=0.0)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy import stats\n",
    "\n",
    "print('Computing SVD with additional features...')\n",
    "svd_extra = TruncatedSVD(n_components=200, random_state=13)\n",
    "svd_extra.fit(fts)\n",
    "\n",
    "print('Computing SVD without additional features...')\n",
    "fts_no_extra = sparse.hstack([fts.tocsc()[:,:-8], np.zeros((fts.shape[0], 8))])\n",
    "svd_no_extra = TruncatedSVD(n_components=200, random_state=13)\n",
    "svd_no_extra.fit(fts_no_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:46:17.012853Z",
     "start_time": "2019-02-28T00:46:17.007610Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix of PCs (should be same size): (200, 262152) vs (200, 262152)\n"
     ]
    }
   ],
   "source": [
    "print('Matrix of PCs (should be same size): %s vs %s' % (str(svd_extra.components_.shape), str(svd_no_extra.components_.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the PCs for both sets of data (with the new features and without), we'll compare *cosine angles* between the two sets of PCs to get a sense of how similar or different they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:59:29.411272Z",
     "start_time": "2019-02-28T00:59:27.943684Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    200.000000\n",
       "mean       1.004655\n",
       "std        0.054193\n",
       "min        0.658319\n",
       "25%        0.987769\n",
       "50%        1.006368\n",
       "75%        1.030209\n",
       "max        1.127439\n",
       "dtype: float64"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "ncomps = svd_extra.components_.shape[0]\n",
    "cosines = [cosine(svd_extra.components_[i], svd_no_extra.components_[i]) for i in range(ncomps)]\n",
    "pd.Series(cosines).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So based on this, we conclude that adding these new features doesn't change the PCs very much at all (cosine similarities are hovering tightly around 1), so:\n",
    "1. The variance in the data is dominated by the text features (as we would expect, since they account for $2^{18}$ of the features)\n",
    "2. It's not worth clustering on this particular feature representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can add these new features after applying PCA to the text features *alone*. Thus we would get a concise representation of the text, and the new features could contribute a larger share of the information in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
