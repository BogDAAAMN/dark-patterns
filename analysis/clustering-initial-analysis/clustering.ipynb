{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl Data Analysis: Clustering\n",
    "\n",
    "This notebook tries various clustering techniques on our web crawl data. It was written for Python 2.7, and assumes it's running on cycles. You can view/edit the notebook remotely as follows:\n",
    "\n",
    "- Clone the GitHub repo to cycles (e.g. spin.cs.princeton.edu)\n",
    "- Start up this notebook. Jupyter is not installed globally, but you can install it locally with pip via `pip install --user jupyter`. Then you can run this notebook in a tmux session: `tmux`, then `cd [this directory]`, then `jupyter notebook --no-browser --port 8889` (note that you can choose whatever port number you want, but we'll assume from here on it's 8889). Copy the URL generated - this is the URL you'll visit in your browser to open the notebook. Then Ctrl-B, D to detach the tmux session, and log out of cycles.\n",
    "- On your local machine, forward your local port 8889 to the remote port 8889 on cycles: `ssh -L 8889:localhost:8889 [netid]@spin.cs.princeton.edu`\n",
    "- Now you can open the notebook in your browser by pasting the link you copied earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T16:41:59.966171Z",
     "start_time": "2019-02-14T16:41:59.956513Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read from database\n",
    "\n",
    "Read the crawl data from the database. Here we read in the `site_visits` and `segments` tables and join them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T16:42:03.894826Z",
     "start_time": "2019-02-14T16:41:59.985330Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "db = '/n/fs/darkpatterns/crawl/2018-12-08_segmentation_pilot2/2018-12-08_segmentation_pilot2.sqlite'\n",
    "con = sqlite3.connect(db)\n",
    "site_visits = pd.read_sql_query('''SELECT * from site_visits''', con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T16:42:03.964473Z",
     "start_time": "2019-02-14T16:42:03.954989Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "print('Number of site visits: %s' % str(site_visits.shape))\n",
    "print('site_visits columns: %s' % str(list(site_visits.columns.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report how many unique domains we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T16:42:04.094475Z",
     "start_time": "2019-02-14T16:42:04.008504Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from urlparse import urlparse\n",
    "\n",
    "site_visits['domain'] = site_visits['site_url'].apply(lambda x: urlparse(x).netloc)\n",
    "grouped = site_visits.groupby(['domain']).count().sort_values('visit_id', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T16:42:04.118945Z",
     "start_time": "2019-02-14T16:42:04.114978Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "print('Number of unique domains: %s' % str(grouped.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T16:48:51.862262Z",
     "start_time": "2019-02-14T16:42:04.183943Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "segments = pd.read_sql_query('''SELECT * from segments''', con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T16:48:55.009321Z",
     "start_time": "2019-02-14T16:48:51.993412Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "segments = segments.reset_index().set_index('visit_id').join(site_visits.reset_index()[['visit_id', 'site_url', 'domain']].set_index('visit_id'), how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T14:31:23.955453Z",
     "start_time": "2019-02-21T14:31:23.940174Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "print('Number of segments: %s' % str(segments.shape))\n",
    "print('segments columns: %s' % str(list(segments.columns.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore `body` tags and null `inner_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T22:37:41.843563Z",
     "start_time": "2019-02-14T22:37:41.540540Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "segments['inner_text'] = segments['inner_text'].str.strip()\n",
    "segments = segments[(segments['node_name'] != 'BODY') & (segments['inner_text'] != '')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a new column for number of newlines in each segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T22:37:42.142436Z",
     "start_time": "2019-02-14T22:37:41.964676Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "segments['newline_count'] = segments['inner_text'].apply(lambda x: len(x.split('\\n')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply some standard techniques in preprocessing string data (ref: [Kdnuggets article](https://www.kdnuggets.com/2017/06/text-clustering-unstructured-data.html)):\n",
    "\n",
    "- Lower case\n",
    "- Replacing numbers with a placeholder, and replace units of measure with a placeholder (e.g kg, sq ft, gb, etc.)\n",
    "- Removing punctuation\n",
    "- Removing excess whitespace\n",
    "- Removing known \"stop words\"/\"stop phrases\"/generic words (articles, conjunctions, etc.)\n",
    "- Stemming (reducing words to their stems, i.e. via Porter's stemming algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T22:37:42.301658Z",
     "start_time": "2019-02-14T22:37:42.263896Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords = stopwords.union(set(string.punctuation))\n",
    "\n",
    "def preprocess(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'\\d+', 'dpnum', s)\n",
    "    s = re.sub(r'[^a-z\\s]', '', s)\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    words = s.split()\n",
    "    words = [stemmer.stem(w) for w in words if len(w) > 0 and w not in stopwords]\n",
    "    \n",
    "    # Optimization to get rid of suffixes (e.g. units of measure)\n",
    "    for i in range(len(words)):\n",
    "        if words[i].startswith('dpnum'):\n",
    "            words[i] = 'dpnum'\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T22:37:42.367429Z",
     "start_time": "2019-02-14T22:37:42.356312Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "s = '''Color   \n",
    "Choose an option\n",
    "Silver\n",
    "Space Gray\n",
    "\n",
    "Size    \n",
    "Choose an option\n",
    "64 GB'''\n",
    "print('ORIGINAL STRING:')\n",
    "print(s)\n",
    "print()\n",
    "print('PREPROCESSED STRING:')\n",
    "print(preprocess(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T22:37:19.361942Z",
     "start_time": "2019-02-14T22:36:36.932293Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "segments['inner_text_processed'] = segments['inner_text'].apply(preprocess)\n",
    "segments['longest_text_processed'] = segments['longest_text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add new columns for length of original text and processed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T22:37:42.626706Z",
     "start_time": "2019-02-14T22:37:42.408876Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "segments['inner_text_length'] = segments['inner_text'].apply(lambda x: len(x))\n",
    "segments['inner_text_processed_length'] = segments['inner_text_processed'].apply(lambda x: len(x))\n",
    "segments['longest_text_length'] = segments['longest_text'].apply(lambda x: len(x))\n",
    "segments['longest_text_processed_length'] = segments['longest_text_processed'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T22:37:42.796446Z",
     "start_time": "2019-02-14T22:37:42.675301Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "new_cols = ['newline_count', 'inner_text_length', 'inner_text_processed_length',\n",
    "            'longest_text_length', 'longest_text_processed_length']\n",
    "for c in new_cols:\n",
    "    print('segments[\\'%s\\'].describe():\\n%s' % (c, segments[c].describe().to_string()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove redundant segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T22:37:46.555646Z",
     "start_time": "2019-02-14T22:37:42.919898Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "segments = segments.groupby(['domain']).apply(lambda x: x.drop_duplicates(subset=['inner_text_processed'], keep='last'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T22:37:46.687040Z",
     "start_time": "2019-02-14T22:37:46.677196Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "print('Number of segments: %s' % str(segments.shape))\n",
    "print('segments columns: %s' % str(list(segments.columns.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a random subset of the data to make clustering more tractable during trial-error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T15:59:40.071889Z",
     "start_time": "2019-02-14T15:59:39.886707Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "segments_orig = segments.copy(deep=False)\n",
    "indices = np.random.choice(np.arange(segments.shape[0]), replace=False, size=25000)\n",
    "segments = segments.iloc[indices,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T16:36:56.992054Z",
     "start_time": "2019-02-14T16:36:56.985455Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "print('segments shape: %s' % str(segments.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feature vectors\n",
    "\n",
    "First we define the a function to tokenize text as we convert text into feature vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T16:49:41.105099Z",
     "start_time": "2019-02-14T16:49:41.083552Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def tokenize(line):\n",
    "    if (line is None):\n",
    "        line = ''\n",
    "    tokens = [stemmer.stem(t) for t in nltk.word_tokenize(line) if len(t) != 0 and t not in stopwords and not t.isdigit()]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now select one of the following cells to run to create a feature representation. Either load from the pre-existing file or recompute the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:27:37.435968Z",
     "start_time": "2019-02-08T15:27:36.153768Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "if os.path.isfile('output/features_bow.npy'):\n",
    "    features = np.load('output/features_bow.npy')\n",
    "    print('Loaded from file')\n",
    "else:\n",
    "    print('No pre-existing file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:27:38.840186Z",
     "start_time": "2019-02-08T15:27:37.446297Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "data = segments['inner_text_processed']\n",
    "vec = CountVectorizer(tokenizer=tokenize, binary=binary_rep, strip_accents='ascii').fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:27:40.296337Z",
     "start_time": "2019-02-08T15:27:38.848496Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "print('Length of vocabulary %s' % str(len(vec.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:27:41.708076Z",
     "start_time": "2019-02-08T15:27:40.307209Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "vec = vec.transform(data)\n",
    "features = normalize(vec, axis=0)\n",
    "np.save('output/features_bow.npy', features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TFIDF\n",
    "\n",
    "Optionally load from a file (enable that cell and disable the others that compute the TFIDF features if you have a file present)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T16:00:52.316962Z",
     "start_time": "2019-02-14T16:00:52.308542Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "if os.path.isfile('output/features_tfidf.npy'):\n",
    "    features = np.load('output/features_tfidf.npy')\n",
    "    print('Loaded from file')\n",
    "else:\n",
    "    print('No pre-existing file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T22:37:50.681169Z",
     "start_time": "2019-02-14T22:37:46.713384Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "data = segments['inner_text_processed']\n",
    "vec = TfidfVectorizer(binary=False, strip_accents='ascii', max_features=1000).fit(data)\n",
    "features = vec.transform(data)\n",
    "np.save('output/features_tfidf.npy', features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T22:37:50.786450Z",
     "start_time": "2019-02-14T22:37:50.778857Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "print('features shape (num_examples, num_features): %s' % str(features.shape))\n",
    "print('Length of vocabulary: %s' % str(len(vec.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T22:37:50.848676Z",
     "start_time": "2019-02-14T22:37:50.823907Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "sorted_vocab = sorted([(t, f) for t, f in vec.vocabulary_.iteritems()], cmp=lambda x, y: x[1] - y[1])\n",
    "print('Vocabulary')\n",
    "print('\\n'.join(['%d: %s' % (f, t) for t, f in sorted_vocab]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute a vector for each segment as follows: compute the word vector for each word in the segment's `inner_text`, and then average over all words in that segment.\n",
    "\n",
    "While it's simple, there are clearly downsides to this approach:\n",
    "\n",
    "- We lose information about word ordering\n",
    "- All words are equally weighted, so words that really characterize the text are not prioritized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T16:59:25.249570Z",
     "start_time": "2019-02-08T16:59:25.201138Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "if os.path.isfile('output/features_wordvec.npy'):\n",
    "    features = np.load('output/features_wordvec.npy')\n",
    "    print('Loaded from file')\n",
    "else:\n",
    "    print('No pre-existing file')\n",
    "    import en_core_web_sm\n",
    "\n",
    "    data = segments['inner_text_processed']\n",
    "    nlp = en_core_web_sm.load()\n",
    "    vecs = []\n",
    "    for doc in nlp.pipe(data.str.replace(r'\\d+', '').astype('unicode').values, batch_size=10000, n_threads=7):\n",
    "        if doc.is_parsed:\n",
    "            vecs.append(doc.vector)\n",
    "        else:\n",
    "            vecs.append(None)\n",
    "    features = np.array(vecs)\n",
    "    np.save('output/features_wordvec.npy', features)\n",
    "    \n",
    "print('features shape: %s' % str(features.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "Try using PCA to reduce the dimension of the data.\n",
    "\n",
    "The feature matrix is expected to be provided with examples in rows (`num_examples` x `num_features`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projected data is given by $U^T X$, where $U$ is matrix with PCs in columns (`orig_dim` x `reduced_dim`), and $X$ is the data matrix with examples in columns (`orig_dim` x `num_examples`). In our case, $U^T$ is `pca.components_` and $X$ is `features.T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T16:59:25.305154Z",
     "start_time": "2019-02-08T16:59:25.274116Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "if os.path.isfile('output/features_proj.npy'):\n",
    "    features = np.load('output/features_proj.npy')\n",
    "    print('Loaded from file')\n",
    "else:\n",
    "    print('No pre-existing file')\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    pca = PCA(n_components=5)\n",
    "    # pca = PCA(tol=10)\n",
    "    pca.fit(features)\n",
    "    \n",
    "    print('Matrix of PCs: %s' % str(pca.components_.shape))\n",
    "    print('Data matrix: %s' % str(features.shape))\n",
    "    print('%d singular values: %s' % (pca.singular_values_.shape[0], str(pca.singular_values_)))\n",
    "    \n",
    "    features = np.dot(pca.components_, features.T)\n",
    "    features = features.T\n",
    "    \n",
    "    np.save('output/features_proj.npy', features)\n",
    "    \n",
    "print('feature matrix shape (after PCA): %s' % str(features.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data in the reduced dimension (assuming new dimension is 3, otherwise this is meaningless)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:39:29.936720Z",
     "start_time": "2019-02-08T15:39:28.494236Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(features[:,0], features[:,1], features[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run one of the following clustering algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T22:27:27.938958Z",
     "start_time": "2019-02-04T22:25:23.988565Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "import fastcluster\n",
    "\n",
    "# featdense = features.todense()\n",
    "distances = distance.pdist(features, metric='cosine')\n",
    "distances = distance.squareform(distances, checks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-04T22:29:23.477Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "clusters = fastcluster.linkage(distances, method='ward', preserve_input=False)\n",
    "np.save('output/hierarchical_linkage_matrix.npy', clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a dendogram of the resulting clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T21:57:35.050335Z",
     "start_time": "2019-02-04T21:57:29.590333Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    clusters,\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=8.,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. DBSCAN clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T15:16:08.506435Z",
     "start_time": "2019-02-21T15:14:36.520272Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.externals.joblib.parallel import parallel_backend\n",
    "\n",
    "with parallel_backend('threading'):\n",
    "    clusterer = DBSCAN(eps=0.0001, min_samples=3, n_jobs=10, metric='euclidean')\n",
    "    cluster_labels = clusterer.fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T15:16:08.532393Z",
     "start_time": "2019-02-21T15:16:08.519083Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "segments['cluster_dbscan'] = pd.Series(cluster_labels.labels_).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T15:16:08.646611Z",
     "start_time": "2019-02-21T15:16:08.539209Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "print('Number of clusters: %d' % len(set(cluster_labels.labels_)))\n",
    "print('segments[\\'cluster_dbscan\\'].value_counts(): \\n %s' % segments['cluster_dbscan'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. HDBSCAN clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T16:39:37.670148Z",
     "start_time": "2019-02-08T16:39:36.168622Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "import hdbscan\n",
    "\n",
    "features = normalize(features, axis=1) # Normalize each segment since using euclidean distance metric\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5, metric='euclidean')\n",
    "cluster_labels = clusterer.fit_predict(features)\n",
    "segments['cluster_hdbscan'] = pd.Series(cluster_labels).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T16:39:39.107312Z",
     "start_time": "2019-02-08T16:39:37.678325Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "print('segments[\\'cluster_hdbscan\\'].value_counts(): \\n %s' % segments['cluster_hdbscan'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Expectation Maximization (EM)\n",
    "\n",
    "Reference/inspiration found [here](https://suif.stanford.edu/~livshits/papers/pdf/uwpc.pdf) and [here](https://www.datascience.com/blog/k-means-alternatives).\n",
    "\n",
    "The form of EM we use here can be thought of as a generalization of K-means clustering, using \"soft\" clusters instead of \"hard\" ones. In relation to K-means: at each iteration, rather than assign every data point to a single cluster, EM computes probability that the point belongs to each cluster, giving a list of probabilities for each data point. Then parameters are updated to maximize the probability of having this distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T07:52:45.411457Z",
     "start_time": "2019-02-15T06:17:01.986408Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "em = GaussianMixture(n_components=100, max_iter=15, tol=0.05, random_state=13, verbose=2, verbose_interval=1)\n",
    "em.fit(features.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T07:56:25.586430Z",
     "start_time": "2019-02-15T07:52:45.416500Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "cluster_labels = em.predict(features.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T07:56:25.613707Z",
     "start_time": "2019-02-15T07:56:25.595954Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "segments['cluster_em'] = pd.Series(cluster_labels).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T15:13:37.402569Z",
     "start_time": "2019-02-21T15:13:37.380558Z"
    }
   },
   "outputs": [],
   "source": [
    "np.save('output/em_cluster_labels.npy', cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T19:10:53.234263Z",
     "start_time": "2019-02-19T19:10:53.196824Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "print('Number of clusters: %d' % len(set(cluster_labels)))\n",
    "print('segments[\\'cluster_em\\'].value_counts(): \\n %s' % segments['cluster_em'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Performance\n",
    "\n",
    "Below we compute a few performance metrics on the clusters to get a sense of how well each method did.\n",
    "\n",
    "### 1. Silhouette Coefficient\n",
    "\n",
    "Ref: https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient\n",
    "\n",
    "Computes for each sample:\n",
    "\n",
    "$$\\frac{b - a}{\\max(a, b)}$$\n",
    "\n",
    "where $a$ is the mean distance between this point and other points in its assigned cluster, and $b$ is the mean distance between this point and other points in the next nearest cluster (based on a distance metric, which is a parameter, such as Euclidean distance).\n",
    "\n",
    "**TL;DR** Ranges between -1 (poorly assigned labels) and +1 (highly dense and separated clusters). In particular, a score near 0 indicates overlapping clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T15:43:41.967885Z",
     "start_time": "2019-02-21T15:17:07.270496Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "s = metrics.silhouette_score(features, segments['cluster'], metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T15:43:41.991220Z",
     "start_time": "2019-02-21T15:43:41.983465Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Silhouette score: %.3f' % s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calinski-Harabaz Index\n",
    "\n",
    "Ref: https://scikit-learn.org/stable/modules/clustering.html#calinski-harabaz-index\n",
    "    \n",
    "Computes the ratio of between-cluster dispersion to within-cluster dispersion. \"Between-cluster dispersion\" is is computed by taking the weighted sum of \"scatter matrices\" between cluster centroids and the centroid of the entire dataset:\n",
    "$$\\sum_q n_q (c_q - c)(c_q - c)^T$$\n",
    "where $n_q$ is the number of points in cluster $q$, $c_q$ is the centroid of cluster $q$, and $c$ is the center of all the data; and then taking the trace.\n",
    "\n",
    "\"Within-cluster dispersion\" for a single cluster is an analogous measure using the scatter matrix over that cluster:\n",
    "$$\\sum_{x \\in \\text{cluster } q} (x - c_q)(x - c_q)^T$$\n",
    "where $c$ is the cluster centroid. Total dispersion is computed by summing these scatter matrices over all clusters and taking the trace.\n",
    "\n",
    "**TL;DR** Larger values are better and indicate denser, more separated clusters. Values near 0 indicate overlapping clusters. This is also the fastest metric to compute, since it's very fast to generate scatter matrices (as opposed to the other metrics, which must compute many pairwise distances)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T15:56:25.905757Z",
     "start_time": "2019-02-21T15:56:23.408147Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "if issparse(features):\n",
    "    chi = metrics.calinski_harabaz_score(features.todense(), segments['cluster'])\n",
    "else:\n",
    "    chi = metrics.calinski_harabaz_score(features, segments['cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T15:56:25.919222Z",
     "start_time": "2019-02-21T15:56:25.913893Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Score: %.3f' % chi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3Davies-Bouldin Index\n",
    "\n",
    "Ref: https://scikit-learn.org/stable/modules/clustering.html#davies-bouldin-index\n",
    "\n",
    "Computes the average \"similarity\" between each cluster and its most similar one. Similarity between clusters $i$ and $j$ is $R_{ij}$ and is computed as:\n",
    "$$R_{ij} = \\frac{s_i + s_j}{d_{ij}}$$\n",
    "where $s_i$ is the average distance from $i$'s centroid to its points (\"cluster diameter\"), and $d_{ij}$ is the distance between cluster centroids. Then the DB index is the average:\n",
    "$$\\frac{1}{k}\\sum_{i} \\max_{j \\neq i} R_{ij}$$\n",
    "\n",
    "**TL;DR** Similarity values closer to 0 indicate better separation of clusters. Thus DB values closer to 0 are better, and larger values indicate sparse and/or overlapping clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T15:59:46.477886Z",
     "start_time": "2019-02-21T15:59:46.438651Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "dbi = davies_bouldin_score(features, segments['cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T15:58:58.615931Z",
     "start_time": "2019-02-21T15:58:58.581902Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Score: %.3f' % dbi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results\n",
    "\n",
    "Produce a CSV file that shows the segments in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T18:53:32.628461Z",
     "start_time": "2019-02-19T18:53:32.609674Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "inner_texts = segments['inner_text']\n",
    "cluster_labels = segments['cluster']\n",
    "urls = segments['site_url']\n",
    "print(\"segments['inner_text'] is %s, segments['cluster'] is %s, segments['site_url'] is %s (should be the same)\" % (str(inner_texts.shape), str(cluster_labels.shape), str(urls.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the segments by cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T18:53:40.324601Z",
     "start_time": "2019-02-19T18:53:33.499682Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "inner_text_by_cluster = defaultdict(lambda: [])\n",
    "url_by_cluster = defaultdict(lambda: [])\n",
    "for i in range(inner_texts.shape[0]):\n",
    "    inner_text_by_cluster[str(cluster_labels[i])].append(inner_texts[i])\n",
    "    url_by_cluster[str(cluster_labels[i])].append(urls[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T18:53:41.597806Z",
     "start_time": "2019-02-19T18:53:40.342214Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import unicodecsv as csv\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = '_'.join(str(datetime.now()).split(' '))\n",
    "outfile = 'output/clusters-%s.csv' % timestamp\n",
    "with open(outfile, 'wb') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for cluster in inner_text_by_cluster.keys():\n",
    "        segments_str = '\\n\\n'.join(['%s:\\n%s' % (u, t) for u, t in zip(url_by_cluster[cluster], inner_text_by_cluster[cluster])])\n",
    "        writer.writerow([cluster, segments_str, url_by_cluster[cluster]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T19:20:54.364008Z",
     "start_time": "2019-02-19T19:20:54.351010Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted_clusters = sorted([(c, texts) for c, texts in inner_text_by_cluster.iteritems()], cmp=lambda x, y: len(y[1]) - len(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T19:39:42.028467Z",
     "start_time": "2019-02-19T19:39:41.997995Z"
    }
   },
   "outputs": [],
   "source": [
    "c, texts = sorted_clusters[54]\n",
    "urls = url_by_cluster[c]\n",
    "print('Cluster %s (length %d)' % (c, len(texts)))\n",
    "print('----------')\n",
    "print('\\n\\n'.join(['%s:\\n%s' % (u, t) for u, t in zip(urls, texts)]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
