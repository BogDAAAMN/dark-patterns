{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl Data Analysis: Clustering\n",
    "\n",
    "This notebook tries various clustering techniques on our web crawl data. It was written for Python 2.7, and assumes it's running on cycles. You can view/edit the notebook remotely as follows:\n",
    "\n",
    "- Clone the GitHub repo to cycles (e.g. spin.cs.princeton.edu)\n",
    "- Start up this notebook. Jupyter is not installed globally, but you can install it locally with pip via `pip install --user jupyter`. Then you can run this notebook in a tmux session: `tmux`, then `cd [this directory]`, then `jupyter notebook --no-browser --port 8889` (note that you can choose whatever port number you want, but we'll assume from here on it's 8889). Copy the URL generated - this is the URL you'll visit in your browser to open the notebook. Then Ctrl-B, D to detach the tmux session, and log out of cycles.\n",
    "- On your local machine, forward your local port 8889 to the remote port 8889 on cycles: `ssh -L 8889:localhost:8889 [netid]@spin.cs.princeton.edu`\n",
    "- Now you can open the notebook in your browser by pasting the link you copied earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T16:54:39.699730Z",
     "start_time": "2019-02-08T16:54:39.276781Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read from database\n",
    "\n",
    "Read the crawl data from the database. Here we read in the `site_visits` and `segments` tables and join them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T16:54:40.534912Z",
     "start_time": "2019-02-08T16:54:39.801450Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# db = '/n/fs/darkpatterns/crawl/2018-12-08_segmentation_pilot2/2018-12-08_segmentation_pilot2.sqlite'\n",
    "db = '/mnt/ssd/amathur/20190206-205000_segmentation_pilot/20190206-205000_segmentation_pilot.sqlite'\n",
    "con = sqlite3.connect(db)\n",
    "site_visits = pd.read_sql_query('''SELECT * from site_visits''', con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T16:54:40.625091Z",
     "start_time": "2019-02-08T16:54:40.617267Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of site visits: (26496, 3)\n",
      "site_visits columns: ['visit_id', 'crawl_id', 'site_url']\n"
     ]
    }
   ],
   "source": [
    "print('Number of site visits: %s' % str(site_visits.shape))\n",
    "print('site_visits columns: %s' % str(list(site_visits.columns.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report how many unique domains we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T16:54:40.763059Z",
     "start_time": "2019-02-08T16:54:40.674900Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from urlparse import urlparse\n",
    "\n",
    "site_visits['domain'] = site_visits['site_url'].apply(lambda x: urlparse(x).netloc)\n",
    "grouped = site_visits.groupby(['domain']).count().sort_values('visit_id', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T16:54:40.788121Z",
     "start_time": "2019-02-08T16:54:40.782724Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique domains: 5799\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique domains: %s' % str(grouped.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T16:58:38.952904Z",
     "start_time": "2019-02-08T16:54:40.859381Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# we do streaming processing instead\n",
    "## segments = pd.read_sql_query('''SELECT * from segments''', con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urlparse import urlparse\n",
    "from collections import defaultdict\n",
    "import binascii\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "DB_NUM = 1  # odin crawl\n",
    "# DB_NUM = 2  # webtap crawl\n",
    "\n",
    "con = sqlite3.connect(db)\n",
    "con.row_factory = sqlite3.Row\n",
    "cur = con.cursor()\n",
    "\n",
    "query = \"\"\"SELECT sv.site_url, sv.visit_id,\n",
    "    sg.id, sg.node_name, sg.node_id, sg.top, sg.left, sg.width, sg.height, \n",
    "    sg.num_buttons, sg.num_imgs, sg.num_anchors,\n",
    "    TRIM(sg.inner_text) as inner_text, TRIM(sg.longest_text) as longest_text\n",
    "    FROM segments as sg LEFT JOIN site_visits as sv\n",
    "    ON sv.visit_id = sg.visit_id WHERE\n",
    "    LOWER(sg.node_name) <> 'body' AND TRIM(sg.inner_text) <> ''\n",
    "    \"\"\"\n",
    "# seen_checksums = defaultdict(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_json = \"segments_odin.json\"\n",
    "# segment_json = \"segments_webtap.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "982it [00:00, 9814.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed segments_odin.json \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1822950it [04:09, 7319.93it/s]"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.remove(segment_json)\n",
    "    print (\"Removed %s \" % segment_json)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "seen_checksums = defaultdict(set)\n",
    "with open(segment_json, \"a\") as f:\n",
    "    for row in tqdm(cur.execute(query)):\n",
    "        inner_processed = row[\"inner_text\"].replace(r'\\d+', 'DPNUM').replace('\\n', ' ').replace('\\r', '')\n",
    "        hostname = urlparse(row[\"site_url\"]).hostname\n",
    "        inner_processed_crc = binascii.crc32(inner_processed.encode('utf-8'))\n",
    "        if inner_processed_crc in seen_checksums[hostname]:\n",
    "            continue\n",
    "        seen_checksums[hostname].add(inner_processed_crc)\n",
    "        row_d = dict(row)\n",
    "        row_d[\"inner_processed\"] = inner_processed\n",
    "        del row_d[\"inner_text\"]\n",
    "        json.dump(dict(row), f)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wc -l segments_odin.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
