{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl Data Analysis: Clustering\n",
    "\n",
    "This notebook tries various clustering techniques on our web crawl data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read from database\n",
    "\n",
    "Read the crawl data from the database. Here we read in the `site_visits` and `segments` tables and join them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "db = '/n/fs/darkpatterns/crawl/2018-12-08_segmentation_pilot2/2018-12-08_segmentation_pilot2.sqlite'\n",
    "con = sqlite3.connect(db)\n",
    "site_visits = pd.read_sql_query('''SELECT * from site_visits''', con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of site visits: %s' % str(site_visits.shape))\n",
    "print('site_visits columns: %s' % str(list(site_visits.columns.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report how many unique domains we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urlparse import urlparse\n",
    "\n",
    "site_visits['domain'] = site_visits['site_url'].apply(lambda x: urlparse(x).netloc)\n",
    "grouped = site_visits.groupby(['domain']).count().sort_values('visit_id', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique domains: %s' % str(grouped.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = pd.read_sql_query('''SELECT * from segments''', con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of segments: %s' % str(segments.shape))\n",
    "print('segments columns: %s' % str(list(segments.columns.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = segments.reset_index().set_index('visit_id').join(site_visits.reset_index()[['visit_id', 'site_url', 'domain']].set_index('visit_id'), how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore `body` tags and null `inner_text`, and add columns for number of newlines and length of `inner_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments['inner_text'] = segments['inner_text'].str.strip()\n",
    "segments = segments[(segments['node_name'] != 'BODY') & (segments['inner_text'] != '')]\n",
    "segments['newline_count'] = segments['inner_text'].apply(lambda x: len(x.split('\\n')))\n",
    "segments['inner_text_length'] = segments['inner_text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('segments[\\'newline_count\\'].describe(): \\n %s' % segments['newline_count'].describe().to_string())\n",
    "print('segments[\\'inner_text_length\\'].describe(): \\n %s' % segments['inner_text_length'].describe().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace numbers with a placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments['inner_text_processed'] = segments['inner_text'].str.replace(r'\\d+', 'DPNUM')\n",
    "segments['longest_text_processed']= segments['longest_text'].str.replace(r'\\d+', 'DPNUM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace redundant segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = segments.groupby(['domain']).apply(lambda x: x.drop_duplicates(subset=['inner_text_processed'], keep='last'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of segments: %s' % str(segments.shape))\n",
    "print('segments columns: %s' % str(list(segments.columns.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feature vectors\n",
    "\n",
    "First we define the a function to tokenize text as we convert text into feature vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def tokenize(line):\n",
    "    if (line is None):\n",
    "        line = ''\n",
    "    tokens = [stemmer.stem(t) for t in nltk.word_tokenize(line) if len(t) != 0 and t not in stopwords and not t.isdigit()]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now select one of the following cells to run to create a feature representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "data = segments['inner_text_processed']\n",
    "vec = CountVectorizer(tokenizer=tokenize, binary=binary_rep, strip_accents='ascii').fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Length of vocabulary %s' % str(len(vec.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = vec.transform(data)\n",
    "features = normalize(vec, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data = segments['inner_text_processed']\n",
    "vec = TfidfVectorizer(tokenizer=tokenize, binary=binary_rep, strip_accents='ascii').fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Length of vocabulary %s' % str(len(vec.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vec.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute a vector for each segment as follows: compute the word vector for each word in the segment's `inner_text`, and then average over all words in that segment.\n",
    "\n",
    "While it's simple, there are clearly downsides to this approach:\n",
    "\n",
    "- We lose information about word ordering\n",
    "- All words are equally weighted, so words that really characterize the text are not prioritized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "data = segments['inner_text_processed']\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "vecs = []\n",
    "for doc in nlp.pipe(data.str.replace(r'\\d+', '').astype('unicode').values, batch_size=10000, n_threads=7):\n",
    "    if doc.is_parsed:\n",
    "        vecs.append(doc.vector)\n",
    "    else:\n",
    "        vecs.append(None)\n",
    "features = np.array(vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "Try using PCA to reduce the dimension of the data.\n",
    "\n",
    "Note that the feature matrix may need to be transposed so that examples are in columns (`num_features` x `num_examples`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "# pca = PCA(tol=1e-5)\n",
    "pca.fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Matrix of PCs: %s' % str(pca.components_.shape))\n",
    "print('Data matrix: %s' % str(features.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projected data is given by $U^T X$, where $U$ is matrix with PCs in columns (`orig_dim` x `reduced_dim`), and $X$ is the data matrix with examples in columns (`orig_dim` x `num_examples`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_proj = pca.components_.dot(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('feature matrix shape (after PCA): %s' % str(features_proj.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run one of the following clustering algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "import fastcluster\n",
    "\n",
    "featdense = features.todense()\n",
    "distances = distance.pdist(featdense, metric='cosine')\n",
    "distances = distance.squareform(distances, checks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = fastcluster.linkage(distances, method='ward', preserve_input=False)\n",
    "np.save('linkage.matrix', clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a dendogram of the resulting clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    clusters,\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=8.,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. DBSCAN clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "clusterer = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=10, metric='cosine')\n",
    "cluster_labels = clusterer.fit(features)\n",
    "segments['cluster'] = pd.Series(cluster_labels.labels_).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('segments[\\'cluster\\'].value_counts(): \\n %s' % segments['cluster'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. HDBSCAN clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "import hdbscan\n",
    "\n",
    "features = normalize(features, axis=1) # Normalize each segment since using euclidean distance metric\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, metric='euclidean')\n",
    "cluster_labels = clusterer.fit_predict(features)\n",
    "segments['cluster'] = pd.Series(cluster_labels).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('segments[\\'cluster\\'].value_counts(): \\n %s' % segments['cluster'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results\n",
    "\n",
    "Produce a CSV file that shows the segments in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_texts = segments['inner_text']\n",
    "cluster_labels = segments['cluster']\n",
    "print(\"segments['inner_text'] is %s, segments['cluster'] is %s (should be the same)\" % (str(inner_texts.shape), str(cluster_labels.shape)))\n",
    "assert inner_texts.shape == cluster_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the segments by cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "segments_by_cluster = defaultdict(lambda: [])\n",
    "for i in range(inner_texts.shape[0]):\n",
    "segments_by_cluster[str(cluster_labels[i])].append(inner_texts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodecsv as csv\n",
    "\n",
    "outfile = 'clusters.csv'\n",
    "with open(outfile, 'wb') as f:\n",
    "writer = csv.writer(f)\n",
    "for cluster, segments in segments_by_cluster.iteritems():\n",
    "    segments_str = '\\n\\n'.join(segments)\n",
    "    writer.writerow([cluster, segments_str])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
