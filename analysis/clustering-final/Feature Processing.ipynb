{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl Data Analysis: Feature Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook tries various feature processing techniques on our web crawl data. It was written for Python 2.7. Before you run this notebook, please make sure you append the two json files from the previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the processed segments from the previous step using the JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_json = '/mnt/ssd/amathur/dark-patterns-output/segments.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing routines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collection of preprocessing routines before feature processing. Add more routines depending on what we would like to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def tokenize(line):\n",
    "    if (line is None):\n",
    "        line = ''\n",
    "    tokens = [stemmer.stem(t) for t in nltk.word_tokenize(line) if len(t) != 0 and t not in stopwords and not t.isdigit()]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words - HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import vstack, hstack\n",
    "import numpy as np\n",
    "\n",
    "vec = HashingVectorizer(tokenizer=tokenize, strip_accents='ascii', norm=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1850895it [59:07, 521.68it/s]\n"
     ]
    }
   ],
   "source": [
    "feature_matrix = None\n",
    "counter = 0\n",
    "\n",
    "with open(segments_json) as f:\n",
    "    seg_matrix_list = []\n",
    "    for line in tqdm(f):\n",
    "        seg = json.loads(line)\n",
    "        seg_matrix = vec.fit_transform([seg['inner_text_processed']])\n",
    "        seg_matrix = hstack((seg_matrix, \n",
    "                             np.array([seg['top']])[:,None],\n",
    "                             np.array([seg['left']])[:,None],\n",
    "                             np.array([seg['height']])[:,None],\n",
    "                             np.array([seg['width']])[:,None],\n",
    "                             np.array([seg['num_buttons']])[:,None],\n",
    "                             np.array([seg['num_imgs']])[:,None],\n",
    "                             np.array([int(seg['num_anchors'])])[:,None]))\n",
    "        \n",
    "        seg_matrix_list.append(seg_matrix)\n",
    "        counter += 1\n",
    "        \n",
    "        if counter % 500 == 0:\n",
    "            feature_matrix = vstack([feature_matrix] + seg_matrix_list)\n",
    "            seg_matrix_list = []\n",
    "            counter = 0\n",
    "        \n",
    "    feature_matrix = vstack([feature_matrix] + seg_matrix_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1850895x1048583 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 25222926 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write the matrix to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import save_npz, load_npz\n",
    "\n",
    "#save_npz(\"/mnt/ssd/amathur/dark-patterns-output/feature_matrix.npz\", feature_matrix)\n",
    "feature_matrix = load_npz(\"/mnt/ssd/amathur/dark-patterns-output/feature_matrix.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reduce the dimensions of this feature matrix to make clustering more tractable. What are the current dimensions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1850895, 1048583)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use TruncatedSVD since it works well with sparse matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=4)\n",
    "result = svd.fit_transform(feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much of the variance do these 4 components when taken together represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999995115319016"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(svd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the dimensions of the returned matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1850895, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write this to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/mnt/ssd/amathur/dark-patterns-output/feature_svd.arr', result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
