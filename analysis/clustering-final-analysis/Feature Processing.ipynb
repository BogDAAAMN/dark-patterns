{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl Data Analysis: Feature Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook tries various feature processing techniques on our web crawl data. It was written for Python 2.7. Before you run this notebook, please make sure you append the two json files from the previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the processed segments from the previous step using the JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_json = '/mnt/ssd/amathur/dark-patterns-output/segments.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing routines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collection of preprocessing routines before feature processing. Add more routines depending on what we would like to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def tokenize(line):\n",
    "    if (line is None):\n",
    "        line = ''\n",
    "    tokens = [stemmer.stem(t) for t in nltk.word_tokenize(line) if len(t) != 0 and t not in stopwords and not t.isdigit()]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words - HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import vstack, hstack\n",
    "import numpy as np\n",
    "from scipy.sparse import save_npz, load_npz\n",
    "\n",
    "vec = HashingVectorizer(tokenizer=tokenize, strip_accents='ascii', n_features=2**18, alternate_sign=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1850895it [29:43, 1037.59it/s]\n"
     ]
    }
   ],
   "source": [
    "text_feature_matrix = None\n",
    "other_feature_matrix = None\n",
    "counter = 0\n",
    "\n",
    "with open(segments_json) as f:\n",
    "    text_matrix_list = []\n",
    "    other_matrix_list = []\n",
    "    \n",
    "    for line in tqdm(f):\n",
    "        segment = json.loads(line)\n",
    "        text_matrix = vec.fit_transform([segment['inner_text_processed']])\n",
    "        other_matrix = np.array([[segment['top'], segment['left'], segment['height'], segment['width']]])\n",
    "        # There are other features we might want to consider (e.g., num_anchors)\n",
    "        \n",
    "        text_matrix_list.append(text_matrix)\n",
    "        other_matrix_list.append(other_matrix)\n",
    "        counter += 1\n",
    "        \n",
    "        if counter % 50000 == 0:\n",
    "            text_feature_matrix = vstack([text_feature_matrix] + text_matrix_list)\n",
    "            other_feature_matrix = vstack([other_feature_matrix] + other_matrix_list)\n",
    "            text_matrix_list = []\n",
    "            other_matrix_list = []\n",
    "            counter = 0\n",
    "        \n",
    "    text_feature_matrix = vstack([text_feature_matrix] + text_matrix_list)\n",
    "    other_feature_matrix = vstack([other_feature_matrix] + other_matrix_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1850895x262144 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 17117540 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1850895x4 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 7335967 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write the matrices to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz(\"/mnt/ssd/amathur/dark-patterns-output/text_feature_matrix_bow.npz\", text_feature_matrix)\n",
    "save_npz(\"/mnt/ssd/amathur/dark-patterns-output/other_matrix.npz\", other_feature_matrix)\n",
    "\n",
    "#text_feature_matrix = load_npz(\"/mnt/ssd/amathur/dark-patterns-output/text_feature_matrix_bow.npz\")\n",
    "#other_feature_matrix = load_npz(\"/mnt/ssd/amathur/dark-patterns-output/other_matrix.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF - TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the bag of words matrix to a TF-IDF structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "tf_idf_text_feature_matrix = transformer.fit_transform(text_feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1850895x262144 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 17117540 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_text_feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write the matrix to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz(\"/mnt/ssd/amathur/dark-patterns-output/text_feature_matrix_tfidf.npz\", tf_idf_text_feature_matrix)\n",
    "#tf_idf_text_feature_matrix = load_npz(\"/mnt/ssd/amathur/dark-patterns-output/text_feature_matrix_tfidf.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1850895x262144 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 17117540 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1850895x4 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 7335967 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1850895x262144 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 17117540 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_text_feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform a PCA on the BoW and Tf-IDF outputs to reduce their dimensions. Starting with the BoW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd_bow = TruncatedSVD(n_components=200)\n",
    "svd_bow_output = svd_bow.fit_transform(text_feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much of the variance do these 50 components when taken together represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5054071724930227"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(svd_bow.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the dimensions of the returned matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1850895, 200)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_bow_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the same reduction for the TF-IDF matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_tfidf = TruncatedSVD(n_components=300)\n",
    "svd_tfidf_output = svd_tfidf.fit_transform(tf_idf_text_feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much of the variance do these 50 components when taken together represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4161701791669506"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(svd_tfidf.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the dimensions of the returned matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1850895, 300)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_tfidf_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach the matrices and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/mnt/ssd/amathur/dark-patterns-output/svd_bow_output.arr', svd_bow_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/mnt/ssd/amathur/dark-patterns-output/svd_tfidf_output.arr', svd_tfidf_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
